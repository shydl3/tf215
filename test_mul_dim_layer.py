#!/usr/bin/env python3
"""
Runtime compatibility test for MulDimLayer on TensorFlow 2.15
Covers: instantiation, forward, gradients, model integration, tf.function.
"""

import os
import sys
import numpy as np
import tensorflow as tf

# è®©è„šæœ¬ä¸é¡¹ç›®é‡Œçš„å¯¼å…¥æ–¹å¼ä¿æŒä¸€è‡´ï¼ˆæŒ‰éœ€ä¿®æ”¹è·¯å¾„ï¼‰
sys.path.append('SemEnr_CSN-JAVA/keras/layers')

from mul_dim_layer_update import MulDimLayer  # ç¡®ä¿æ–‡ä»¶å/ç±»åä¸ä¸Šé¢ä¸€è‡´


def test_mul_dim_layer():
    print("TensorFlow:", tf.__version__)
    print("Testing MulDimLayer...")

    # åŸºæœ¬å½¢çŠ¶
    B, T, D = 4, 6, 32
    x = np.random.randn(B, T, D).astype(np.float32)

    try:
        # 1) å®ä¾‹åŒ– & å‰å‘
        print("\n1) Instantiation & Forward")
        layer = MulDimLayer()
        y = layer(x)
        print("   Output shape:", y.shape)
        assert y.shape == (B, T, D)

        # éªŒè¯æ•°å€¼ï¼šåº”ç­‰äºè¾“å…¥ * sqrt(D)
        scale = np.sqrt(D).astype(np.float32)
        diff = np.max(np.abs(y.numpy() - (x * scale)))
        print("   Max abs diff vs expected:", float(diff))
        assert diff < 1e-6

        # 2) æ¢¯åº¦æ£€æŸ¥ï¼ˆæŒ‚åœ¨ä¸€ä¸ªå°æ¨¡å‹ä¸Šï¼‰
        print("\n2) Gradient Check")
        with tf.GradientTape() as tape:
            tape.watch(layer.trainable_variables)  # æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œè¿™é‡Œä¸ºç©ºåˆ—è¡¨
            y2 = layer(x)
            loss = tf.reduce_mean(tf.square(y2))
        grads = tape.gradient(loss, layer.trainable_variables)
        # è¯¥å±‚æ—  trainable_variablesï¼Œç¡®ä¿ä¸ä¼šæŠ¥é”™å³å¯
        print("   Trainable vars:", len(layer.trainable_variables))
        assert grads == [] or grads is None
        print("   Gradient path OK (no trainable vars)")

        # 3) Keras æ¨¡å‹é›†æˆï¼ˆcompile/predictï¼‰
        print("\n3) Keras Model Integration")
        inp = tf.keras.layers.Input(shape=(T, D))
        out = MulDimLayer()(inp)
        model = tf.keras.Model(inp, out)
        model.compile(optimizer='adam', loss='mse')
        pred = model.predict(x, verbose=0)
        print("   Predict OK, shape:", pred.shape)
        assert pred.shape == (B, T, D)

        # 4) @tf.function è®­ç»ƒæ­¥
        print("\n4) @tf.function Train Step")
        opt = tf.keras.optimizers.Adam(1e-3)

        @tf.function
        def train_step(batch):
            with tf.GradientTape() as tape:
                out = layer(batch, training=True)
                loss = tf.reduce_mean(tf.square(out))
            # è¯¥å±‚æ— å¯è®­ç»ƒå‚æ•°ï¼Œè¿™é‡Œåªæ˜¯éªŒè¯è·¯å¾„ä¸æŠ¥é”™
            grads = tape.gradient(loss, layer.trainable_variables)
            if grads:
                opt.apply_gradients(zip(grads, layer.trainable_variables))
            return loss

        loss_val = train_step(tf.convert_to_tensor(x))
        print(f"   Train step OK, loss={loss_val.numpy():.6f}")

        # 5) ä¿å­˜/åŠ è½½ï¼ˆSavedModelï¼‰
        print("\n5) Save/Load Roundtrip (SavedModel)")
        tmp_dir = "./_tmp_mul_dim_savedmodel"
        model.save(tmp_dir, overwrite=True)
        reloaded = tf.keras.models.load_model(tmp_dir)
        pred2 = reloaded.predict(x, verbose=0)
        assert np.allclose(pred, pred2, atol=1e-6)
        print("   Save/Load OK")

        print("\nğŸ‰ MulDimLayer all checks passed!")
        return True

    except Exception as e:
        print("âŒ Test failed:", str(e))
        import traceback; traceback.print_exc()
        return False


if __name__ == "__main__":
    ok = test_mul_dim_layer()
    print("\nResult:", "PASS" if ok else "FAIL")
